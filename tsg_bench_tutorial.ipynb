{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSG-Bench Tutorial: Understanding Temporal-Spatial Scene Graph Evaluation\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of the TSG-Bench repository, demonstrating:\n",
    "- How scene graphs are structured\n",
    "- The 4 evaluation tasks\n",
    "- How to use OpenAI models for evaluation\n",
    "- Where evaluation code lives and how it works\n",
    "- Running hands-on examples\n",
    "\n",
    "**Repository:** TSG-Bench - First unified benchmark for scene graph understanding and generation\n",
    "\n",
    "**Website:** https://tsg-bench.netlify.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import necessary libraries and add the repo to our path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/jtu9/sgg/tsg-bench\n",
      "Python path updated: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Add repo to path\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_path))\n",
    "\n",
    "print(f\"Working directory: {repo_path}\")\n",
    "print(f\"Python path updated: {repo_path in [Path(p) for p in sys.path]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repository Overview\n",
    "\n",
    "### What is TSG-Bench?\n",
    "\n",
    "TSG-Bench evaluates Large Language Models on their ability to:\n",
    "1. **Generate** scene graphs from textual descriptions\n",
    "2. **Understand** scene graphs and reason over them\n",
    "\n",
    "### The 4 Evaluation Tasks:\n",
    "\n",
    "| Task | Type | Description | Dataset Size | Metrics |\n",
    "|------|------|-------------|--------------|----------|\n",
    "| **SA-SGG** | Generation | Single-action scene graph generation | 1,188 samples | P/R/F1 |\n",
    "| **MA-SGG** | Generation | Multi-action scene graph generation | 853 samples | P/R/F1 |\n",
    "| **SGQA** | Understanding | Scene graph question answering | 99 samples (500+ QA pairs) | Accuracy |\n",
    "| **SGDS** | Understanding | Scene graph description selection | 249 samples | Accuracy |\n",
    "\n",
    "### Repository Structure:\n",
    "\n",
    "```\n",
    "tsg-bench/\n",
    "├── evaluation/          # 4 task evaluation scripts\n",
    "│   ├── generation/      # sa-sgg.py, ma-sgg.py\n",
    "│   └── understanding/   # sgqa.py, sgds.py\n",
    "├── models/models.py     # LLM wrappers (OpenAI, Anthropic, etc.)\n",
    "├── resource/\n",
    "│   ├── dataset/         # JSONL datasets for all tasks\n",
    "│   └── prompts/         # Task-specific prompt templates\n",
    "└── utils/               # Config and path utilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Scene Graphs\n",
    "\n",
    "### What is a Scene Graph?\n",
    "\n",
    "A scene graph represents actions and spatial relationships as a graph of **triplets**:\n",
    "```\n",
    "[source_node, edge_type, target_node]\n",
    "```\n",
    "\n",
    "### Node Types:\n",
    "- **person** (always the starting node)\n",
    "- **Actions:** pick-up, place, tighten, drill, etc.\n",
    "- **Objects:** screwdriver, bowl, wood, clamp, etc.\n",
    "- **Hands:** hand1 (left), hand2 (right)\n",
    "\n",
    "### Edge Types:\n",
    "- **verb:** connects person → action\n",
    "- **dobj:** direct object (action → object)\n",
    "- **Prepositions:** with, from, into, on, under, etc.\n",
    "\n",
    "### Example:\n",
    "**Text:** \"Pick up the screwdriver with your left hand\"\n",
    "\n",
    "**Scene Graph:**\n",
    "```python\n",
    "[\n",
    "    [\"person\", \"verb\", \"pick-up\"],\n",
    "    [\"pick-up\", \"dobj\", \"screwdriver\"],\n",
    "    [\"pick-up\", \"with\", \"hand1\"]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading and Exploring Datasets\n",
    "\n",
    "Let's load examples from each task dataset to see their structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SA-SGG Dataset (Single-Action Scene Graph Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SA-SGG samples: 1188\n",
      "\n",
      "================================================================================\n",
      "Example SA-SGG Sample:\n",
      "================================================================================\n",
      "\n",
      "Data ID: e9be1118-a5cf-4431-b2e8-e3edcfa9f949\n",
      "\n",
      "Context: The task began by preparing the necessary tools, picking up the screwdriver and the screw. The screw was then positioned and secured into the wood using the screwdriver. Once the screw was firmly in p...\n",
      "\n",
      "Target Sentence: The fastener was then secured tightly with both hands to ensure stability.\n",
      "\n",
      "Ground Truth Scene Graph:\n",
      "[['person', 'verb', 'tighten'],\n",
      " ['tighten', 'dobj', 'clamp'],\n",
      " ['tighten', 'with', 'hand1'],\n",
      " ['tighten', 'with', 'hand2']]\n",
      "\n",
      "Available Objects: ['person', 'clamp', 'hand1', 'hand2', 'screwdriver', 'wood', 'wood-piece', 'wood-pieces', 'workbench']...\n",
      "Available Verbs: ['align', 'drill', 'hold', 'loosen', 'pick-up', 'place', 'position', 'put-down', 'release', 'remove']...\n",
      "Available Relationships: ['dobj', 'from', 'into', 'on', 'with', 'verb', 'with']...\n"
     ]
    }
   ],
   "source": [
    "# Load SA-SGG dataset\n",
    "sa_sgg_path = repo_path / \"resource\" / \"dataset\" / \"generation\" / \"sa-sgg.jsonl\"\n",
    "\n",
    "with open(sa_sgg_path, 'r') as f:\n",
    "    sa_sgg_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total SA-SGG samples: {len(sa_sgg_samples)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example SA-SGG Sample:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "example = sa_sgg_samples[0]\n",
    "print(f\"\\nData ID: {example['data_id']}\")\n",
    "print(f\"\\nContext: {example['context'][:200]}...\")\n",
    "print(f\"\\nTarget Sentence: {example['target_sentence']}\")\n",
    "print(f\"\\nGround Truth Scene Graph:\")\n",
    "pprint(example['graphs'][0]['triplets'])\n",
    "print(f\"\\nAvailable Objects: {example['mandatory_space']['object'][:10]}...\")\n",
    "print(f\"Available Verbs: {example['mandatory_space']['verb'][:10]}...\")\n",
    "print(f\"Available Relationships: {example['mandatory_space']['relationship'][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MA-SGG Dataset (Multi-Action Scene Graph Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MA-SGG samples: 853\n",
      "\n",
      "================================================================================\n",
      "Example MA-SGG Sample (Multiple Actions):\n",
      "================================================================================\n",
      "\n",
      "Target Sentence: The task began by preparing the necessary tools, picking up the screwdriver and the screw.\n",
      "\n",
      "Number of Scene Graphs: 2\n",
      "\n",
      "Scene Graph 1 (Action 14):\n",
      "[['person', 'verb', 'pick-up'],\n",
      " ['pick-up', 'dobj', 'screwdriver'],\n",
      " ['pick-up', 'with', 'hand1']]\n",
      "\n",
      "Scene Graph 2 (Action 13):\n",
      "[['person', 'verb', 'pick-up'],\n",
      " ['pick-up', 'dobj', 'screw'],\n",
      " ['pick-up', 'with', 'hand2']]\n"
     ]
    }
   ],
   "source": [
    "# Load MA-SGG dataset\n",
    "ma_sgg_path = repo_path / \"resource\" / \"dataset\" / \"generation\" / \"ma-sgg.jsonl\"\n",
    "\n",
    "with open(ma_sgg_path, 'r') as f:\n",
    "    ma_sgg_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total MA-SGG samples: {len(ma_sgg_samples)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example MA-SGG Sample (Multiple Actions):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "example = ma_sgg_samples[0]\n",
    "print(f\"\\nTarget Sentence: {example['target_sentence']}\")\n",
    "print(f\"\\nNumber of Scene Graphs: {len(example['graphs'])}\")\n",
    "for i, graph in enumerate(example['graphs']):\n",
    "    print(f\"\\nScene Graph {i+1} (Action {graph['action_id']}):\")\n",
    "    pprint(graph['triplets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SGQA Dataset (Scene Graph Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SGQA samples: 100\n",
      "Total QA pairs: 500\n",
      "\n",
      "================================================================================\n",
      "Example SGQA Sample:\n",
      "================================================================================\n",
      "\n",
      "Context Scene Graphs (11 graphs):\n",
      "\n",
      "Graph 1:\n",
      "[['pick-up', 'with', 'hand1'],\n",
      " ['pick-up', 'with', 'hand2'],\n",
      " ['mop-stick', 'from', 'floor'],\n",
      " ['person', 'verb', 'pick-up'],\n",
      " ['pick-up', 'dobj', 'mop-stick']]\n",
      "\n",
      "Graph 2:\n",
      "[['sweep', 'with', 'hand1'],\n",
      " ['sweep', 'with', 'hand2'],\n",
      " ['sweep', 'with', 'mop-stick'],\n",
      " ['sweep', 'dobj', 'floor'],\n",
      " ['sweep', 'in', 'car'],\n",
      " ['person', 'verb', 'sweep']]\n",
      "\n",
      "Question-Answer Pairs (5 pairs):\n",
      "\n",
      "Q1: What object was picked up before sweeping the floor?\n",
      "A1: mop-stick\n",
      "\n",
      "Q2: Which location did the person interact with after using the cloth?\n",
      "A2: wall\n",
      "\n",
      "Q3: What object was handled immediately after opening the cabinet?\n",
      "A3: cloth\n"
     ]
    }
   ],
   "source": [
    "# Load SGQA dataset\n",
    "sgqa_path = repo_path / \"resource\" / \"dataset\" / \"understanding\" / \"sgqa.jsonl\"\n",
    "\n",
    "with open(sgqa_path, 'r') as f:\n",
    "    sgqa_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total SGQA samples: {len(sgqa_samples)}\")\n",
    "\n",
    "# Count total QA pairs\n",
    "total_qa = sum(len(sample['qa_pairs']) for sample in sgqa_samples)\n",
    "print(f\"Total QA pairs: {total_qa}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example SGQA Sample:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "example = sgqa_samples[0]\n",
    "print(f\"\\nContext Scene Graphs ({len(example['context_graphs'])} graphs):\")\n",
    "for i, graph in enumerate(example['context_graphs'][:2]):\n",
    "    print(f\"\\nGraph {i+1}:\")\n",
    "    pprint(graph)\n",
    "\n",
    "print(f\"\\nQuestion-Answer Pairs ({len(example['qa_pairs'])} pairs):\")\n",
    "for i, qa in enumerate(example['qa_pairs'][:3]):\n",
    "    print(f\"\\nQ{i+1}: {qa['Q']}\")\n",
    "    print(f\"A{i+1}: {qa['A']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 SGDS Dataset (Scene Graph Description Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SGDS samples: 250\n",
      "\n",
      "================================================================================\n",
      "Example SGDS Sample:\n",
      "================================================================================\n",
      "\n",
      "Target Scene Graph (triplets):\n",
      "[['roll', 'dobj', 'dough'],\n",
      " ['roll', 'with', 'roller'],\n",
      " ['roll', 'with', 'hand1'],\n",
      " ['roll', 'with', 'hand2'],\n",
      " ['person', 'verb', 'roll']]\n",
      "\n",
      "Correct Answer Position: 1 (0-indexed)\n",
      "\n",
      "Candidate Descriptions (5 options):\n",
      "[A] This step was skipped to avoid achieving the desired consistency. \n",
      "[B] This step was repeated to achieve the desired consistency. <-- CORRECT\n",
      "[C] This step was repeated to disrupt the desired consistency. \n",
      "[D] This step was altered to prevent the desired consistency. \n",
      "[E] This step was ignored to ensure the undesired consistency. \n"
     ]
    }
   ],
   "source": [
    "# Load SGDS dataset\n",
    "sgds_path = repo_path / \"resource\" / \"dataset\" / \"understanding\" / \"sgds.jsonl\"\n",
    "\n",
    "with open(sgds_path, 'r') as f:\n",
    "    sgds_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total SGDS samples: {len(sgds_samples)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example SGDS Sample:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "example = sgds_samples[0]\n",
    "print(f\"\\nTarget Scene Graph (triplets):\")\n",
    "pprint(example['triplet'])\n",
    "\n",
    "print(f\"\\nCorrect Answer Position: {example['position']} (0-indexed)\")\n",
    "print(f\"\\nCandidate Descriptions (5 options):\")\n",
    "for i, desc in enumerate(example['variations']):\n",
    "    marker = \"<-- CORRECT\" if i == example['position'] else \"\"\n",
    "    print(f\"[{chr(65+i)}] {desc} {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup - OpenAI\n",
    "\n",
    "Let's configure and test the OpenAI model wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config file found at conf.d/config.yaml\n",
      "\n",
      "Note: Make sure your OpenAI API key is configured in this file:\n",
      "\n",
      "openai:\n",
      "  key: your_api_key_here\n"
     ]
    }
   ],
   "source": [
    "# Import model classes\n",
    "from models.models import GPT4o, GPT4oMini\n",
    "\n",
    "# Check if config exists\n",
    "config_path = repo_path / \"conf.d\" / \"config.yaml\"\n",
    "if config_path.exists():\n",
    "    print(\"✓ Config file found at conf.d/config.yaml\")\n",
    "    print(\"\\nNote: Make sure your OpenAI API key is configured in this file:\")\n",
    "    print(\"\"\"\\nopenai:\n",
    "  key: your_api_key_here\"\"\")\n",
    "else:\n",
    "    print(\"⚠ Config file not found!\")\n",
    "    print(\"Please create conf.d/config.yaml with your OpenAI API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI models initialized successfully\n",
      "✗ Error initializing models: 'GPT4o' object has no attribute 'model_name'\n",
      "Make sure your OpenAI API key is configured in conf.d/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "try:\n",
    "    gpt4o = GPT4o()\n",
    "    gpt4o_mini = GPT4oMini()\n",
    "    print(\"✓ OpenAI models initialized successfully\")\n",
    "    print(f\"  - GPT-4o: {gpt4o.model_name}\")\n",
    "    print(f\"  - GPT-4o-mini: {gpt4o_mini.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error initializing models: {e}\")\n",
    "    print(\"Make sure your OpenAI API key is configured in conf.d/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model invocation successful!\n",
      "\n",
      "Prompt: What is a scene graph? Answer in one sentence.\n",
      "\n",
      "Response: A scene graph is a data structure that represents the spatial and hierarchical relationships between objects in a scene, often used in computer graphics and visualization to manage and render complex environments efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Test model invocation\n",
    "test_prompt = \"What is a scene graph? Answer in one sentence.\"\n",
    "\n",
    "try:\n",
    "    response = gpt4o_mini.invoke(test_prompt)\n",
    "    print(\"✓ Model invocation successful!\")\n",
    "    print(f\"\\nPrompt: {test_prompt}\")\n",
    "    print(f\"\\nResponse: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error invoking model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task Deep Dive: SA-SGG (Single-Action Scene Graph Generation)\n",
    "\n",
    "### Where is the evaluation code?\n",
    "**Location:** `evaluation/generation/sa-sgg.py`\n",
    "\n",
    "### Key Classes:\n",
    "1. **GraphGeneration** - Formats prompts and invokes model\n",
    "2. **GraphScorer** - Parses responses and calculates metrics\n",
    "3. **GraphEvaluator** - Orchestrates the evaluation pipeline\n",
    "\n",
    "### Method:\n",
    "1. Load sample with context, target sentence, available nodes/edges\n",
    "2. Format prompt using template from `resource/prompts/sa-sgg.txt`\n",
    "3. Invoke model to generate scene graph triplets\n",
    "4. Parse response (format: `node -> edge -> node`)\n",
    "5. Calculate Precision, Recall, F1 vs ground truth\n",
    "\n",
    "**Note:** The evaluation files have hyphens in their names (e.g., `sa-sgg.py`), so we use `importlib` to import them (Python doesn't allow hyphens in standard import statements).\n",
    "\n",
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SA-SGG module imported successfully\n",
      "\n",
      "SA-SGG Prompt Template (first 500 chars):\n",
      "================================================================================\n",
      "You are an AI model tasked with generating a scene graph based on a given sentence, adhering to specific rules for the graph, nodes, and edges, while considering the provided context, available nodes, and available edges.\n",
      "\n",
      "Rules for Scene Graph Representation:\n",
      "1. A graph is composed of one or more triplets of nodes and edges.\n",
      "2. A triplet starts with a node and another node is connected by an edge. (Format: node -> edge -> node)\n",
      "3. Each triplet is split with a new line.\n",
      "4. There must be a triple...\n",
      "\n",
      "[Full prompt is 2451 characters]\n"
     ]
    }
   ],
   "source": [
    "# Import SA-SGG evaluation classes using importlib\n",
    "# (Files have hyphens in names, so we can't use standard import)\n",
    "import importlib.util\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"sa_sgg\",\n",
    "    str(repo_path / \"evaluation\" / \"generation\" / \"sa-sgg.py\")\n",
    ")\n",
    "sa_sgg_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(sa_sgg_module)\n",
    "\n",
    "# Now we can access the classes\n",
    "GraphGeneration = sa_sgg_module.GraphGeneration\n",
    "GraphScorer = sa_sgg_module.GraphScorer\n",
    "\n",
    "print(\"✓ SA-SGG module imported successfully\")\n",
    "\n",
    "# Load prompt template\n",
    "prompt_path = repo_path / \"resource\" / \"prompts\" / \"sa-sgg.txt\"\n",
    "with open(prompt_path, 'r') as f:\n",
    "    sa_sgg_prompt_template = f.read()\n",
    "\n",
    "print(\"\\nSA-SGG Prompt Template (first 500 chars):\")\n",
    "print(\"=\"*80)\n",
    "print(sa_sgg_prompt_template[:500] + \"...\")\n",
    "print(\"\\n[Full prompt is {} characters]\".format(len(sa_sgg_prompt_template)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sample:\n",
      "================================================================================\n",
      "Target Sentence: Finally, the rag was placed back on the floor.\n",
      "\n",
      "Ground Truth Triplets:\n",
      "[['put-down', 'with', 'hand1'],\n",
      " ['put-down', 'dobj', 'rag'],\n",
      " ['put-down', 'on', 'floor'],\n",
      " ['person', 'verb', 'put-down']]\n",
      "\n",
      "================================================================================\n",
      "Inputs prepared for model:\n",
      "================================================================================\n",
      "Context: The book collection was organized with both hands to ensure everything was in its proper place. A book was relocated from the floor to the bookshelf with both hands, ensuring it was positioned correct...\n",
      "Available nodes: person, book, bookshelf, brush, dust, floor, hand1, hand2, rag, shelf...\n",
      "Available edges: dobj, from, on, verb, with\n",
      "Available verbs: ['align', 'arrange', 'check', 'hold', 'pick', 'pick-up', 'put', 'put-down', 'remove', 'take']...\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate scene graph for one sample\n",
    "sample = sa_sgg_samples[10]  # Pick an interesting example\n",
    "\n",
    "print(\"Input Sample:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Target Sentence: {sample['target_sentence']}\")\n",
    "print(f\"\\nGround Truth Triplets:\")\n",
    "pprint(sample['graphs'][0]['triplets'])\n",
    "\n",
    "# Prepare inputs\n",
    "context = sample['context']\n",
    "target_sentence = sample['target_sentence']\n",
    "available_nodes = \", \".join(sample['mandatory_space']['object'])\n",
    "available_edges = \", \".join(sample['mandatory_space']['relationship'])\n",
    "verbs = sample['mandatory_space']['verb']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Inputs prepared for model:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Context: {context[:200]}...\")\n",
    "print(f\"Available nodes: {available_nodes[:100]}...\")\n",
    "print(f\"Available edges: {available_edges}\")\n",
    "print(f\"Available verbs: {verbs[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Model Response:\n",
      "================================================================================\n",
      "person -> verb -> put  \n",
      "rag -> dobj -> floor  \n",
      "rag -> from -> floor\n",
      "\n",
      "================================================================================\n",
      "Parsed Scene Graphs:\n",
      "================================================================================\n",
      "[['person', 'verb', 'put'], ['rag', 'dobj', 'floor'], ['rag', 'from', 'floor']]\n",
      "\n",
      "================================================================================\n",
      "Evaluation Metrics:\n",
      "================================================================================\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Invoke model\n",
    "try:\n",
    "    graph_gen = GraphGeneration(gpt4o_mini)\n",
    "    response = graph_gen.invoke(\n",
    "        context=context,\n",
    "        target_sentence=target_sentence,\n",
    "        available_nodes=available_nodes,\n",
    "        available_edges=available_edges,\n",
    "        verbs=verbs\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Model Response:\")\n",
    "    print(\"=\"*80)\n",
    "    print(response)\n",
    "    \n",
    "    # Parse and score\n",
    "    scorer = GraphScorer()\n",
    "    parsed_scene_graphs = scorer.parse_response(response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Parsed Scene Graphs:\")\n",
    "    print(\"=\"*80)\n",
    "    if parsed_scene_graphs:\n",
    "        predicted_triplets = parsed_scene_graphs[0]  # Take first scene graph\n",
    "        pprint(predicted_triplets)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = scorer.calculate_scores(\n",
    "            sample['graphs'][0]['triplets'],\n",
    "            predicted_triplets\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Evaluation Metrics:\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.3f}\")\n",
    "    else:\n",
    "        print(\"No valid scene graph parsed from response\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during generation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task Deep Dive: MA-SGG (Multi-Action Scene Graph Generation)\n",
    "\n",
    "### Where is the evaluation code?\n",
    "**Location:** `evaluation/generation/ma-sgg.py`\n",
    "\n",
    "### Key Difference from SA-SGG:\n",
    "- Model must generate **exactly N scene graphs** (one per action)\n",
    "- Requires implicit action segmentation\n",
    "- Graphs separated by blank lines in output\n",
    "\n",
    "### Method:\n",
    "Same as SA-SGG but with multiple graphs per sample.\n",
    "\n",
    "**Note:** Using `importlib` to import the hyphenated file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MA-SGG module imported successfully\n",
      "\n",
      "MA-SGG Prompt Template (first 500 chars):\n",
      "================================================================================\n",
      "You are an AI model tasked with generating scene graphs based on a given sentence. Your goal is to create exactly the specified number of scene graphs by extracting meaningful relationships between entities, actions, and objects while ensuring that the scene graphs represent actions that would visually appear in a scene.\n",
      "\n",
      "Rules for Generating Multiple Scene Graphs:\n",
      "1. Generate precisely {num_scene_graphs} scene graphs—no more, no less.\n",
      "2. Each scene graph must depict an action that would be expl...\n",
      "\n",
      "================================================================================\n",
      "Input Sample:\n",
      "================================================================================\n",
      "Target Sentence: The wood piece was carefully adjusted using both hands.\n",
      "\n",
      "Number of Actions: 2\n",
      "\n",
      "Ground Truth Scene Graphs:\n",
      "\n",
      "Graph 1:\n",
      "[['person', 'verb', 'pick-up'],\n",
      " ['pick-up', 'dobj', 'wood-piece'],\n",
      " ['pick-up', 'with', 'hand1'],\n",
      " ['pick-up', 'with', 'hand2']]\n",
      "\n",
      "Graph 2:\n",
      "[['person', 'verb', 'align'],\n",
      " ['align', 'dobj', 'wood-piece'],\n",
      " ['align', 'with', 'hand1'],\n",
      " ['align', 'with', 'hand2']]\n"
     ]
    }
   ],
   "source": [
    "# Import MA-SGG evaluation classes using importlib\n",
    "spec_ma = importlib.util.spec_from_file_location(\n",
    "    \"ma_sgg\",\n",
    "    str(repo_path / \"evaluation\" / \"generation\" / \"ma-sgg.py\")\n",
    ")\n",
    "ma_sgg_module = importlib.util.module_from_spec(spec_ma)\n",
    "spec_ma.loader.exec_module(ma_sgg_module)\n",
    "\n",
    "MAGraphGeneration = ma_sgg_module.GraphGeneration\n",
    "MAGraphScorer = ma_sgg_module.GraphScorer\n",
    "\n",
    "print(\"✓ MA-SGG module imported successfully\")\n",
    "\n",
    "# Load prompt template\n",
    "ma_prompt_path = repo_path / \"resource\" / \"prompts\" / \"ma-sgg.txt\"\n",
    "with open(ma_prompt_path, 'r') as f:\n",
    "    ma_sgg_prompt_template = f.read()\n",
    "\n",
    "print(\"\\nMA-SGG Prompt Template (first 500 chars):\")\n",
    "print(\"=\"*80)\n",
    "print(ma_sgg_prompt_template[:500] + \"...\")\n",
    "\n",
    "# Example with multiple actions\n",
    "sample = ma_sgg_samples[5]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Input Sample:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Target Sentence: {sample['target_sentence']}\")\n",
    "print(f\"\\nNumber of Actions: {len(sample['graphs'])}\")\n",
    "print(f\"\\nGround Truth Scene Graphs:\")\n",
    "for i, graph in enumerate(sample['graphs']):\n",
    "    print(f\"\\nGraph {i+1}:\")\n",
    "    pprint(graph['triplets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Task Deep Dive: SGQA (Scene Graph Question Answering)\n",
    "\n",
    "### Where is the evaluation code?\n",
    "**Location:** `evaluation/understanding/sgqa.py`\n",
    "\n",
    "### Key Classes:\n",
    "1. **QA** - Prompts model with scene graphs + question\n",
    "2. **QADataLoader** - Loads and structures QA pairs\n",
    "3. **QAEvaluator** - Processes questions and calculates accuracy\n",
    "\n",
    "### Method:\n",
    "1. Load QA pairs with context scene graphs\n",
    "2. Format prompt with scene graphs + question\n",
    "3. Extract answer using regex `\\[(.*?)\\]`\n",
    "4. Compare with ground truth (case-insensitive exact match)\n",
    "5. Calculate accuracy\n",
    "\n",
    "**Note:** Using `importlib` to import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SGQA module imported successfully\n",
      "\n",
      "SGQA Prompt Template:\n",
      "================================================================================\n",
      "You are a highly advanced language model specialized in answering questions based on a given scene graph and question. Your task is to analyze the scene graph and provide the correct answer in a single word. Your output must strictly follow the format [answer], and nothing else should be printed. Ensure that your answer is concise, accurate, and matches the format exactly.\n",
      "\n",
      "Scene Graph: {scene_graph}\n",
      "Question: {question}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Input:\n",
      "================================================================================\n",
      "Context Graphs: 19 scene graphs\n",
      "\n",
      "Graph 1: [['turn-on', 'dobj', 'tap'], ['turn-on', 'with', 'hand1'], ['person', 'verb', 'turn-on']]\n",
      "\n",
      "Graph 2: [['wash', 'dobj', 'onion'], ['onion', 'in', 'hand2'], ['wash', 'under', 'water'], ['person', 'verb', 'wash']]\n",
      "\n",
      "Graph 3: [['turn-off', 'dobj', 'tap'], ['turn-off', 'with', 'hand1'], ['person', 'verb', 'turn-off']]\n",
      "\n",
      "Question: What object was first manipulated before water flow began?\n",
      "Ground Truth Answer: tap\n"
     ]
    }
   ],
   "source": [
    "# Import SGQA evaluation classes using importlib\n",
    "spec_sgqa = importlib.util.spec_from_file_location(\n",
    "    \"sgqa\",\n",
    "    str(repo_path / \"evaluation\" / \"understanding\" / \"sgqa.py\")\n",
    ")\n",
    "sgqa_module = importlib.util.module_from_spec(spec_sgqa)\n",
    "spec_sgqa.loader.exec_module(sgqa_module)\n",
    "\n",
    "QA = sgqa_module.QA\n",
    "\n",
    "print(\"✓ SGQA module imported successfully\")\n",
    "\n",
    "# Load prompt template\n",
    "sgqa_prompt_path = repo_path / \"resource\" / \"prompts\" / \"sgqa.txt\"\n",
    "with open(sgqa_prompt_path, 'r') as f:\n",
    "    sgqa_prompt_template = f.read()\n",
    "\n",
    "print(\"\\nSGQA Prompt Template:\")\n",
    "print(\"=\"*80)\n",
    "print(sgqa_prompt_template)\n",
    "\n",
    "# Example QA\n",
    "sample = sgqa_samples[2]\n",
    "qa_pair = sample['qa_pairs'][0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Input:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Context Graphs: {len(sample['context_graphs'])} scene graphs\")\n",
    "for i, graph in enumerate(sample['context_graphs'][:3]):\n",
    "    print(f\"\\nGraph {i+1}: {graph}\")\n",
    "print(f\"\\nQuestion: {qa_pair['Q']}\")\n",
    "print(f\"Ground Truth Answer: {qa_pair['A']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input:\n",
      "================================================================================\n",
      "Scene Graphs: [[['turn-on', 'dobj', 'tap'], ['turn-on', 'with', 'hand1'], ['person', 'verb', 'turn-on']], [['wash', 'dobj', 'onion'], ['onion', 'in', 'hand2'], ['wash', 'under', 'water'], ['person', 'verb', 'wash']], [['turn-off', 'dobj', 'tap'], ['turn-off', 'with', 'hand1'], ['person', 'verb', 'turn-off']], [['pick-up', 'dobj', 'pot'], ['pick-up', 'with', 'hand1'], ['person', 'verb', 'pick-up']], [['transfer', 'dobj', 'onion'], ['onion', 'from', 'board'], ['transfer', 'into', 'pot'], ['transfer', 'with', 'h...\n",
      "\n",
      "Question: What object was first manipulated before water flow began?\n",
      "\n",
      "================================================================================\n",
      "Model Response:\n",
      "================================================================================\n",
      "Predicted Answer: onion\n",
      "\n",
      "================================================================================\n",
      "Evaluation:\n",
      "================================================================================\n",
      "Predicted: onion\n",
      "Ground Truth: tap\n",
      "Correct: False\n"
     ]
    }
   ],
   "source": [
    "# Test SGQA\n",
    "try:\n",
    "    qa = QA(gpt4o_mini)\n",
    "    \n",
    "    # Format scene graphs as string\n",
    "    scene_graph_str = str(sample['context_graphs'])\n",
    "    \n",
    "    print(\"Full Input:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Scene Graphs: {scene_graph_str[:500]}...\")\n",
    "    print(f\"\\nQuestion: {qa_pair['Q']}\")\n",
    "    \n",
    "    # Invoke model\n",
    "    predicted_answer = qa.invoke(\n",
    "        scene_graph=scene_graph_str,\n",
    "        question=qa_pair['Q']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Model Response:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Evaluation:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Predicted: {predicted_answer}\")\n",
    "    print(f\"Ground Truth: {qa_pair['A']}\")\n",
    "    print(f\"Correct: {predicted_answer.lower() == qa_pair['A'].lower()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during QA: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Task Deep Dive: SGDS (Scene Graph Description Selection)\n",
    "\n",
    "### Where is the evaluation code?\n",
    "**Location:** `evaluation/understanding/sgds.py`\n",
    "\n",
    "### Key Classes:\n",
    "1. **SceneGraphToText** - Prompts model with scene graph + candidates\n",
    "2. **SceneGraphEvaluator** - Evaluates across dataset\n",
    "\n",
    "### Method:\n",
    "1. Load samples with target scene graph + 5 candidate descriptions\n",
    "2. Model selects best matching description (A-E)\n",
    "3. Parse answer letter, convert to index\n",
    "4. Compare with ground truth position\n",
    "5. Calculate accuracy\n",
    "\n",
    "**Note:** Using `importlib` to import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SGDS module imported successfully\n",
      "\n",
      "SGDS Prompt Template:\n",
      "================================================================================\n",
      "You are an AI that analyzes a Scene Graph based on the context and select the best text description of it among the given candidates.\n",
      "\n",
      "1. Input:\n",
      "   - Context: A list of scene graphs representing the preceding context.\n",
      "     - Each graph is composed of a set of triplets `[node1, edge, node2]`. `node1` and `node2` are one of person, action, object and hand. `edge` represents the relationship between them (e.g., `verb`, `dobj`, `from`, `with`).\n",
      "   - Target Scene Graph: A set of triplets that should be described into text correctly.\n",
      "   - Description Candidates: Candidates of sentence descriptions of the Target Scene Graph based on the Context.\n",
      "\n",
      "2. Task:\n",
      "   - Determine which description best matches the Target Scene Graph.\n",
      "\n",
      "3. Output:  \n",
      "   - Be sure to choose only one letter of the matching description.  \n",
      "   - Do not output any additional text or explanation. Only the letter in [ ] (e.g., [A]).\n",
      "\n",
      "Key rules of edges in a triplet:\n",
      "   - `verb` describes the action performed by `person`.\n",
      "   - `dobj` links the action to its direct object (`node2`).\n",
      "   - Other edges like `from` and `with` describe spatial relationships between nodes.\n",
      "\n",
      "Input:\n",
      "- Context: {context}\n",
      "- Target Scene Graph: {triplet}\n",
      "- Description Candidates: \n",
      "{sentences}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Input:\n",
      "================================================================================\n",
      "Target Scene Graph:\n",
      "[['person', 'verb', 'apply'],\n",
      " ['apply', 'dobj', 'glue'],\n",
      " ['apply', 'with', 'brush'],\n",
      " ['apply', 'with', 'hand1'],\n",
      " ['apply', 'onto', 'bolt']]\n",
      "\n",
      "Candidate Descriptions:\n",
      "[A] Adhesive was then removed from the fastener using a scraper held in one hand. \n",
      "[B] Paint was then applied onto the wall using a roller held in one hand. \n",
      "[C] Glue was then spread onto the paper using a brush held in both hands. \n",
      "[D] Adhesive was then wiped off the fastener using a cloth held in one hand. \n",
      "[E] Adhesive was then used onto the fastener using an applicator held in one hand. <-- CORRECT\n"
     ]
    }
   ],
   "source": [
    "# Import SGDS evaluation classes using importlib\n",
    "spec_sgds = importlib.util.spec_from_file_location(\n",
    "    \"sgds\",\n",
    "    str(repo_path / \"evaluation\" / \"understanding\" / \"sgds.py\")\n",
    ")\n",
    "sgds_module = importlib.util.module_from_spec(spec_sgds)\n",
    "spec_sgds.loader.exec_module(sgds_module)\n",
    "\n",
    "SceneGraphToText = sgds_module.SceneGraphToText\n",
    "\n",
    "print(\"✓ SGDS module imported successfully\")\n",
    "\n",
    "# Load prompt template\n",
    "sgds_prompt_path = repo_path / \"resource\" / \"prompts\" / \"sgds.txt\"\n",
    "with open(sgds_prompt_path, 'r') as f:\n",
    "    sgds_prompt_template = f.read()\n",
    "\n",
    "print(\"\\nSGDS Prompt Template:\")\n",
    "print(\"=\"*80)\n",
    "print(sgds_prompt_template)\n",
    "\n",
    "# Example\n",
    "sample = sgds_samples[5]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Input:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Target Scene Graph:\")\n",
    "pprint(sample['triplet'])\n",
    "print(f\"\\nCandidate Descriptions:\")\n",
    "for i, desc in enumerate(sample['variations']):\n",
    "    marker = \"<-- CORRECT\" if i == sample['position'] else \"\"\n",
    "    print(f\"[{chr(65+i)}] {desc} {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Input:\n",
      "================================================================================\n",
      "Target Scene Graph: [['person', 'verb', 'apply'], ['apply', 'dobj', 'glue'], ['apply', 'with', 'brush'], ['apply', 'with', 'hand1'], ['apply', 'onto', 'bolt']]\n",
      "\n",
      "Candidate Descriptions:\n",
      "[A] Adhesive was then removed from the fastener using a scraper held in one hand.\n",
      "[B] Paint was then applied onto the wall using a roller held in one hand.\n",
      "[C] Glue was then spread onto the paper using a brush held in both hands.\n",
      "[D] Adhesive was then wiped off the fastener using a cloth held in one hand.\n",
      "[E] Adhesive was then used onto the fastener using an applicator held in one hand.\n",
      "\n",
      "================================================================================\n",
      "Model Response:\n",
      "================================================================================\n",
      "[E]\n",
      "Error during SGDS: name 're' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/srv/local/shared/temp/tmp1/jtu9/tmp/ipykernel_848332/668971126.py\", line 28, in <module>\n",
      "    match = re.search(r'\\[([A-E])\\]', response)\n",
      "NameError: name 're' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Test SGDS\n",
    "try:\n",
    "    sgds = SceneGraphToText(gpt4o_mini)\n",
    "    \n",
    "    # Format sentences (variations) as string\n",
    "    sentences_str = \"\\n\".join([f\"[{chr(65+i)}] {desc}\" for i, desc in enumerate(sample['variations'])])\n",
    "    triplet_str = str(sample['triplet'])\n",
    "    context_str = str(sample.get('context_graphs', []))\n",
    "    \n",
    "    print(\"Full Input:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Target Scene Graph: {triplet_str}\")\n",
    "    print(f\"\\nCandidate Descriptions:\\n{sentences_str}\")\n",
    "    \n",
    "    # Invoke model\n",
    "    response = sgds.invoke(\n",
    "        sentences=sentences_str,\n",
    "        triplet=triplet_str,\n",
    "        context_graphs=context_str\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Model Response:\")\n",
    "    print(\"=\"*80)\n",
    "    print(response)\n",
    "    \n",
    "    # Parse answer\n",
    "    match = re.search(r'\\[([A-E])\\]', response)\n",
    "    predicted_letter = match.group(1) if match else None\n",
    "    predicted_index = ord(predicted_letter) - ord('A') if predicted_letter else None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Evaluation:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Predicted: {predicted_letter} (index {predicted_index})\")\n",
    "    print(f\"Ground Truth: {chr(65 + sample['position'])} (index {sample['position']})\")\n",
    "    print(f\"Correct: {predicted_index == sample['position'] if predicted_index is not None else False}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during SGDS: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Running Full Evaluations\n",
    "\n",
    "The evaluation scripts can be run directly from command line:\n",
    "\n",
    "```bash\n",
    "# SA-SGG evaluation\n",
    "python evaluation/generation/sa-sgg.py\n",
    "\n",
    "# MA-SGG evaluation\n",
    "python evaluation/generation/ma-sgg.py\n",
    "\n",
    "# SGQA evaluation\n",
    "python evaluation/understanding/sgqa.py\n",
    "\n",
    "# SGDS evaluation\n",
    "python evaluation/understanding/sgds.py\n",
    "```\n",
    "\n",
    "Each script:\n",
    "1. Loops through all configured models\n",
    "2. Uses ThreadPoolExecutor for parallel API calls (5-15 workers)\n",
    "3. Calculates metrics across entire dataset\n",
    "4. Prints results to console\n",
    "\n",
    "### Evaluation Code Structure:\n",
    "\n",
    "All evaluation scripts follow similar pattern:\n",
    "```python\n",
    "class TaskGeneration/TaskQA:\n",
    "    def __init__(self, model)\n",
    "    def prompt_format(...)  # Format prompt with template\n",
    "    def invoke(...)         # Call model\n",
    "\n",
    "class TaskScorer:\n",
    "    def parse_response(...)  # Extract predictions\n",
    "    def score(...)           # Calculate metrics\n",
    "\n",
    "class TaskEvaluator:\n",
    "    def evaluate(...)        # Run full evaluation\n",
    "    def _parallel_process()  # ThreadPoolExecutor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Mini-Evaluation Example\n",
    "\n",
    "Let's run a small evaluation on 5 samples from SA-SGG to see the complete pipeline in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SA-SGG evaluation on 5 samples with GPT-4o-mini...\n",
      "================================================================================\n",
      "Error during evaluation: expected str, bytes or os.PathLike object, not list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/srv/local/shared/temp/tmp1/jtu9/tmp/ipykernel_848332/3310588118.py\", line 13, in <module>\n",
      "    results = evaluator.evaluate(mini_dataset)\n",
      "  File \"/home/jtu9/sgg/tsg-bench/evaluation/generation/sa-sgg.py\", line 151, in evaluate\n",
      "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
      "TypeError: expected str, bytes or os.PathLike object, not list\n"
     ]
    }
   ],
   "source": [
    "# Mini SA-SGG evaluation\n",
    "# Use GraphEvaluator from the sa_sgg_module we imported earlier\n",
    "GraphEvaluator = sa_sgg_module.GraphEvaluator\n",
    "\n",
    "# Take first 5 samples\n",
    "mini_dataset = sa_sgg_samples[:5]\n",
    "\n",
    "print(f\"Running SA-SGG evaluation on {len(mini_dataset)} samples with GPT-4o-mini...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    evaluator = GraphEvaluator(gpt4o_mini)\n",
    "    results = evaluator.evaluate(mini_dataset)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Average Precision: {results['avg_precision']:.3f}\")\n",
    "    print(f\"Average Recall: {results['avg_recall']:.3f}\")\n",
    "    print(f\"Average F1 Score: {results['avg_f1']:.3f}\")\n",
    "    print(f\"\\nSamples evaluated: {len(results['samples'])}\")\n",
    "    \n",
    "    print(\"\\nPer-sample breakdown:\")\n",
    "    for i, sample_result in enumerate(results['samples']):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  P: {sample_result['precision']:.3f}, R: {sample_result['recall']:.3f}, F1: {sample_result['f1']:.3f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Understanding Evaluation Metrics\n",
    "\n",
    "### Generation Tasks (SA-SGG, MA-SGG):\n",
    "\n",
    "**Precision:** How many predicted triplets are correct?\n",
    "```python\n",
    "precision = correct_triplets / predicted_triplets\n",
    "```\n",
    "\n",
    "**Recall:** How many ground truth triplets were found?\n",
    "```python\n",
    "recall = correct_triplets / ground_truth_triplets\n",
    "```\n",
    "\n",
    "**F1 Score:** Harmonic mean of precision and recall\n",
    "```python\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "```\n",
    "\n",
    "**Macro-averaging:** Average metrics across all samples/graphs\n",
    "\n",
    "### Understanding Tasks (SGQA, SGDS):\n",
    "\n",
    "**Accuracy:** Percentage of correct predictions\n",
    "```python\n",
    "accuracy = correct_predictions / total_predictions\n",
    "```\n",
    "\n",
    "**Matching:** Case-insensitive exact string match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Insights and Next Steps\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Scene Graph Structure:**\n",
    "   - Triplets: `[source_node, edge_type, target_node]`\n",
    "   - Always starts with `person` node\n",
    "   - Captures both actions and spatial relationships\n",
    "\n",
    "2. **Four Evaluation Tasks:**\n",
    "   - **SA-SGG:** Single action generation (1,188 samples)\n",
    "   - **MA-SGG:** Multi-action generation (853 samples)\n",
    "   - **SGQA:** Question answering (500+ QA pairs)\n",
    "   - **SGDS:** Description selection (249 samples)\n",
    "\n",
    "3. **Evaluation Pipeline:**\n",
    "   - Load data from `resource/dataset/`\n",
    "   - Format prompts using templates from `resource/prompts/`\n",
    "   - Invoke models via unified interface in `models/models.py`\n",
    "   - Parse responses and calculate metrics\n",
    "   - Use parallel processing for efficiency\n",
    "\n",
    "4. **Code Locations:**\n",
    "   - **SA-SGG:** `evaluation/generation/sa-sgg.py`\n",
    "   - **MA-SGG:** `evaluation/generation/ma-sgg.py`\n",
    "   - **SGQA:** `evaluation/understanding/sgqa.py`\n",
    "   - **SGDS:** `evaluation/understanding/sgds.py`\n",
    "   - **Models:** `models/models.py`\n",
    "\n",
    "### How to Extend:\n",
    "\n",
    "1. **Add New Models:**\n",
    "   ```python\n",
    "   class NewModel(LLM):\n",
    "       def __init__(self):\n",
    "           self.model_name = \"new-model\"\n",
    "       \n",
    "       def invoke(self, message):\n",
    "           # Your implementation\n",
    "           return response\n",
    "   ```\n",
    "\n",
    "2. **Customize Prompts:**\n",
    "   - Edit files in `resource/prompts/`\n",
    "   - Modify `prompt_format()` methods in evaluation scripts\n",
    "\n",
    "3. **Save Results:**\n",
    "   - Uncomment `_save_results()` in evaluation scripts\n",
    "   - Or redirect stdout: `python sa-sgg.py > results.txt`\n",
    "\n",
    "4. **Batch Processing:**\n",
    "   - Adjust `max_workers` in ThreadPoolExecutor\n",
    "   - Balance API rate limits vs speed\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Website:** https://tsg-bench.netlify.app/\n",
    "- **Dataset:** `resource/dataset/`\n",
    "- **Prompts:** `resource/prompts/`\n",
    "- **Config:** `conf.d/config.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✓ How scene graphs represent actions and spatial relationships as triplets  \n",
    "✓ The 4 evaluation tasks and their datasets  \n",
    "✓ How to configure and use OpenAI models  \n",
    "✓ Where evaluation code lives and how it works  \n",
    "✓ The complete evaluation pipeline from prompt to metrics  \n",
    "✓ How to run evaluations and interpret results  \n",
    "\n",
    "**Next steps:**\n",
    "- Run full evaluations on all datasets\n",
    "- Compare different models (GPT-4o vs GPT-4o-mini vs Claude, etc.)\n",
    "- Analyze error patterns\n",
    "- Customize prompts for better performance\n",
    "\n",
    "Happy experimenting with TSG-Bench! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsg-bench)",
   "language": "python",
   "name": "tsg-bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
