{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EpiMine Tutorial: Building Hierarchical Event Structures from Scene Graphs\n",
    "\n",
    "This notebook demonstrates step-by-step how to build a hierarchical event structure from a video scene graph using the EpiMine methodology.\n",
    "\n",
    "**EpiMine** is an unsupervised episode detection framework that detects episode boundaries through **co-occurrence pattern shifts** rather than semantic similarity alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Core Concepts\n",
    "\n",
    "### What is EpiMine?\n",
    "\n",
    "EpiMine (ACL 2025) is an unsupervised method for detecting episodes in sequential data. The key insight:\n",
    "\n",
    "> **Episodes cannot be detected using semantic similarity alone—they require analyzing shifts in term co-occurrence patterns.**\n",
    "\n",
    "### Core Philosophy\n",
    "\n",
    "1. **Discriminative Key Terms**: Identify terms that distinguish foreground (current sample) from background (all samples)\n",
    "2. **Co-occurrence as Structure Signal**: Terms that co-occur form \"who-did-what\" relationships; shifts indicate boundaries\n",
    "3. **Statistical Thresholds**: Use `mean ± 1σ` to adaptively detect boundaries\n",
    "4. **LLM Refinement**: Use LLM to generate human-readable episode descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (3.10.7)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /srv/local/shared/temp/tmp1/jtu9/envs/tsg-bench/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# pip install matplotlib and seaborn if not already installed\n",
    "! pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from pprint import pprint\n",
    "\n",
    "# For visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    HAS_VIZ = True\n",
    "except ImportError:\n",
    "    HAS_VIZ = False\n",
    "    print(\"matplotlib/seaborn not available - skipping visualizations\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Load Sample Data\n",
    "\n",
    "We'll load a single sample from the SGQA dataset to demonstrate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 samples\n",
      "\n",
      "Sample ID: 19cc4e42-39bb-41f9-b9de-9f2940eed6a2\n",
      "Number of actions: 11\n"
     ]
    }
   ],
   "source": [
    "# Path to SGQA dataset\n",
    "SGQA_PATH = Path(\"/home/jtu9/sgg/tsg-bench/resource/dataset/understanding/sgqa.jsonl\")\n",
    "\n",
    "# Load all samples\n",
    "all_samples = []\n",
    "with open(SGQA_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            all_samples.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(all_samples)} samples\")\n",
    "\n",
    "# Use first sample for demonstration\n",
    "sample = all_samples[0]\n",
    "action_sequence = sample[\"context_graphs\"]\n",
    "\n",
    "print(f\"\\nSample ID: {sample['data_id']}\")\n",
    "print(f\"Number of actions: {len(action_sequence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ACTION SEQUENCE (Scene Graphs)\n",
      "================================================================================\n",
      "\n",
      "Action 0 (pick-up):\n",
      "  ['pick-up', 'with', 'hand1']\n",
      "  ['pick-up', 'with', 'hand2']\n",
      "  ['mop-stick', 'from', 'floor']\n",
      "  ['person', 'verb', 'pick-up']\n",
      "  ['pick-up', 'dobj', 'mop-stick']\n",
      "\n",
      "Action 1 (sweep):\n",
      "  ['sweep', 'with', 'hand1']\n",
      "  ['sweep', 'with', 'hand2']\n",
      "  ['sweep', 'with', 'mop-stick']\n",
      "  ['sweep', 'dobj', 'floor']\n",
      "  ['sweep', 'in', 'car']\n",
      "  ['person', 'verb', 'sweep']\n",
      "\n",
      "Action 2 (close):\n",
      "  ['close', 'with', 'hand1']\n",
      "  ['close', 'dobj', 'door']\n",
      "  ['person', 'verb', 'close']\n",
      "\n",
      "Action 3 (place):\n",
      "  ['place', 'with', 'hand2']\n",
      "  ['place', 'dobj', 'mop-stick']\n",
      "  ['place', 'on', 'floor']\n",
      "  ['person', 'verb', 'place']\n",
      "\n",
      "Action 4 (open):\n",
      "  ['open', 'dobj', 'door']\n",
      "  ['open', 'with', 'hand2']\n",
      "  ['person', 'verb', 'open']\n",
      "\n",
      "Action 5 (put):\n",
      "  ['put', 'dobj', 'cloth']\n",
      "  ['put', 'inside', 'car']\n",
      "  ['put', 'with', 'hand1']\n",
      "  ['person', 'verb', 'put']\n",
      "\n",
      "Action 6 (move):\n",
      "  ['person', 'verb', 'move']\n",
      "  ['move', 'to', 'cabinet']\n",
      "\n",
      "Action 7 (open):\n",
      "  ['open', 'dobj', 'cabinet']\n",
      "  ['open', 'with', 'hand1']\n",
      "  ['person', 'verb', 'open']\n",
      "\n",
      "Action 8 (pick-up):\n",
      "  ['pick-up', 'with', 'hand1']\n",
      "  ['pick-up', 'dobj', 'cloth']\n",
      "  ['person', 'verb', 'pick-up']\n",
      "\n",
      "Action 9 (move):\n",
      "  ['move', 'to', 'wall']\n",
      "  ['hand1', 'in', 'cloth']\n",
      "  ['move', 'with', 'hand1']\n",
      "  ['person', 'verb', 'move']\n",
      "\n",
      "Action 10 (pick):\n",
      "  ['pick', 'with', 'hand2']\n",
      "  ['pick', 'from', 'wall']\n",
      "  ['person', 'verb', 'pick']\n",
      "  ['pick', 'dobj', 'mop-stick']\n"
     ]
    }
   ],
   "source": [
    "# Visualize the action sequence\n",
    "print(\"=\" * 80)\n",
    "print(\"ACTION SEQUENCE (Scene Graphs)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def get_action_verb(action_graph):\n",
    "    \"\"\"Extract main verb from action graph.\"\"\"\n",
    "    for triplet in action_graph:\n",
    "        if len(triplet) >= 3 and triplet[1] in {\"verb\", \"verbs\"}:\n",
    "            return triplet[2]\n",
    "    return \"unknown\"\n",
    "\n",
    "for i, action_graph in enumerate(action_sequence):\n",
    "    verb = get_action_verb(action_graph)\n",
    "    print(f\"\\nAction {i} ({verb}):\")\n",
    "    for triplet in action_graph:\n",
    "        print(f\"  {triplet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Build Background Dataset\n",
    "\n",
    "The **background dataset** contains all actions from the entire SGQA dataset. This provides the \"general distribution\" against which we can identify discriminative terms.\n",
    "\n",
    "**Why do we need background?**\n",
    "- Terms that appear frequently in both foreground AND background are not discriminative (e.g., \"person\")\n",
    "- Terms that appear frequently in foreground but rarely in background ARE discriminative (e.g., \"mop-stick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background dataset: 2546 action graphs\n",
      "\n",
      "Unique terms in background: 560\n",
      "\n",
      "Top 10 most common terms:\n",
      "  person: 2545\n",
      "  verb: 2527\n",
      "  dobj: 2525\n",
      "  with: 2446\n",
      "  hand1: 2037\n",
      "  hand2: 1088\n",
      "  pick-up: 596\n",
      "  on: 423\n",
      "  from: 307\n",
      "  place: 304\n"
     ]
    }
   ],
   "source": [
    "def extract_terms_from_action(action_graph: List[List[str]]) -> Set[str]:\n",
    "    \"\"\"Extract all terms from an action graph.\"\"\"\n",
    "    terms = set()\n",
    "    for triplet in action_graph:\n",
    "        for term in triplet:\n",
    "            terms.add(term)\n",
    "    return terms\n",
    "\n",
    "# Build background: all actions from all samples\n",
    "background_actions = []\n",
    "for s in all_samples:\n",
    "    background_actions.extend(s[\"context_graphs\"])\n",
    "\n",
    "print(f\"Background dataset: {len(background_actions)} action graphs\")\n",
    "\n",
    "# Count term frequencies in background\n",
    "term_counts_bg = Counter()\n",
    "for action_graph in background_actions:\n",
    "    terms = extract_terms_from_action(action_graph)\n",
    "    term_counts_bg.update(terms)\n",
    "\n",
    "print(f\"\\nUnique terms in background: {len(term_counts_bg)}\")\n",
    "print(f\"\\nTop 10 most common terms:\")\n",
    "for term, count in term_counts_bg.most_common(10):\n",
    "    print(f\"  {term}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Understanding Key Parameters\n",
    "\n",
    "### 5.1 What is `min_freq`?\n",
    "\n",
    "**`min_freq`** is the minimum number of times a term must appear in the foreground (current sample) to be considered as a key term.\n",
    "\n",
    "| Implementation | min_freq | Reason |\n",
    "|---------------|----------|--------|\n",
    "| Original EpiMine (News) | 5 | Large vocabulary, many rare words |\n",
    "| SGQA Adaptation | 2 | Small, fixed vocabulary |\n",
    "\n",
    "**Why filter by min_freq?**\n",
    "- Prevents rare/one-off terms from affecting episode detection\n",
    "- Ensures only **frequent** terms are considered as key terms\n",
    "- Terms appearing ≤ min_freq times return salience = -1 (filtered out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term frequencies in FOREGROUND (current sample):\n",
      "==================================================\n",
      "  person: 11 → ✓ KEPT\n",
      "  verb: 11 → ✓ KEPT\n",
      "  with: 10 → ✓ KEPT\n",
      "  dobj: 9 → ✓ KEPT\n",
      "  hand1: 7 → ✓ KEPT\n",
      "  hand2: 5 → ✓ KEPT\n",
      "  mop-stick: 4 → ✓ KEPT\n",
      "  floor: 3 → ✓ KEPT\n",
      "  cloth: 3 → ✓ KEPT\n",
      "  pick-up: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  from: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  car: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  in: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  door: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  open: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  to: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  cabinet: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  move: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  wall: 2 → ✗ FILTERED (≤ min_freq=2)\n",
      "  sweep: 1 → ✗ FILTERED (≤ min_freq=2)\n",
      "  close: 1 → ✗ FILTERED (≤ min_freq=2)\n",
      "  place: 1 → ✗ FILTERED (≤ min_freq=2)\n",
      "  on: 1 → ✗ FILTERED (≤ min_freq=2)\n",
      "  put: 1 → ✗ FILTERED (≤ min_freq=2)\n",
      "  inside: 1 → ✗ FILTERED (≤ min_freq=2)\n",
      "  pick: 1 → ✗ FILTERED (≤ min_freq=2)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate min_freq effect\n",
    "foreground = action_sequence  # Current sample's actions\n",
    "\n",
    "# Count term frequencies in foreground\n",
    "term_counts_fg = Counter()\n",
    "for action_graph in foreground:\n",
    "    terms = extract_terms_from_action(action_graph)\n",
    "    term_counts_fg.update(terms)\n",
    "\n",
    "print(\"Term frequencies in FOREGROUND (current sample):\")\n",
    "print(\"=\" * 50)\n",
    "for term, count in sorted(term_counts_fg.items(), key=lambda x: -x[1]):\n",
    "    status = \"✓ KEPT\" if count > 2 else \"✗ FILTERED (≤ min_freq=2)\"\n",
    "    print(f\"  {term}: {count} → {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 What is Term Expansion (cosine ≥ 0.75)?\n",
    "\n",
    "**Term Expansion** is a technique used in the **original EpiMine** (news domain) to handle synonyms and semantically similar words.\n",
    "\n",
    "#### The Problem (in News Domain)\n",
    "News articles use varied vocabulary:\n",
    "- \"killed\", \"murdered\", \"slain\", \"assassinated\" → same concept\n",
    "- Without expansion, these are treated as separate terms\n",
    "\n",
    "#### The Solution: Two-Stage Expansion\n",
    "\n",
    "```\n",
    "Stage 1: Expand key terms (threshold = 0.75)\n",
    "─────────────────────────────────────────────\n",
    "1. Compute salience for all vocabulary words\n",
    "2. Keep high-salience words as \"key terms\"\n",
    "3. For each excluded word:\n",
    "   - Compute cosine similarity to all key terms\n",
    "   - If max similarity ≥ 0.75, add to key terms\n",
    "\n",
    "Stage 2: Cluster similar terms (threshold = 0.85)\n",
    "─────────────────────────────────────────────────\n",
    "1. Compute pairwise cosine similarity between key terms\n",
    "2. Cluster terms with similarity ≥ 0.85\n",
    "3. Treat clustered terms as ONE concept for co-occurrence\n",
    "```\n",
    "\n",
    "#### Why SGQA Doesn't Need Term Expansion\n",
    "\n",
    "| Aspect | News Domain | SGQA Scene Graphs |\n",
    "|--------|-------------|-------------------|\n",
    "| Vocabulary | Open-ended (any word) | Fixed (predefined verbs, objects) |\n",
    "| Synonyms | Common (killed/murdered) | None (actions are standardized) |\n",
    "| Term matching | Fuzzy (need expansion) | Exact (triplet terms) |\n",
    "\n",
    "**Scene graphs use a controlled vocabulary** — there's only one way to say \"pick-up\", so no expansion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERM EXPANSION EXAMPLE (Original EpiMine - News Domain)\n",
      "============================================================\n",
      "\n",
      "Key terms identified: ['killed', 'attack', 'explosion']\n",
      "\n",
      "Excluded terms with similarity to 'killed':\n",
      "  murdered  → cosine = 0.82 ≥ 0.75 → ADD to key terms ✓\n",
      "  slain     → cosine = 0.78 ≥ 0.75 → ADD to key terms ✓\n",
      "  injured   → cosine = 0.61 < 0.75 → SKIP ✗\n",
      "\n",
      "After clustering (threshold = 0.85):\n",
      "  Cluster 0: ['killed', 'murdered', 'slain'] → treated as ONE concept\n",
      "  Cluster 1: ['attack']\n",
      "  Cluster 2: ['explosion']\n",
      "\n",
      "------------------------------------------------------------\n",
      "SGQA: No expansion needed (fixed vocabulary, no synonyms)\n"
     ]
    }
   ],
   "source": [
    "# Illustrate what term expansion would do (conceptually)\n",
    "print(\"TERM EXPANSION EXAMPLE (Original EpiMine - News Domain)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Key terms identified: ['killed', 'attack', 'explosion']\")\n",
    "print()\n",
    "print(\"Excluded terms with similarity to 'killed':\")\n",
    "print(\"  murdered  → cosine = 0.82 ≥ 0.75 → ADD to key terms ✓\")\n",
    "print(\"  slain     → cosine = 0.78 ≥ 0.75 → ADD to key terms ✓\")\n",
    "print(\"  injured   → cosine = 0.61 < 0.75 → SKIP ✗\")\n",
    "print()\n",
    "print(\"After clustering (threshold = 0.85):\")\n",
    "print(\"  Cluster 0: ['killed', 'murdered', 'slain'] → treated as ONE concept\")\n",
    "print(\"  Cluster 1: ['attack']\")\n",
    "print(\"  Cluster 2: ['explosion']\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "print(\"SGQA: No expansion needed (fixed vocabulary, no synonyms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Compute Salience\n",
    "\n",
    "**Salience** measures how discriminative a term is for the current sample (foreground) compared to the general distribution (background).\n",
    "\n",
    "### Formula\n",
    "```\n",
    "salience(term) = (1 + log(fg_count)²) × log(bg_total / bg_count)\n",
    "```\n",
    "\n",
    "- **`(1 + log(fg_count)²)`**: Boost terms that appear repeatedly in foreground\n",
    "- **`log(bg_total / bg_count)`**: IDF component — penalize common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALIENCE SCORES\n",
      "============================================================\n",
      "Term                 FG Count   BG Count   Salience  \n",
      "------------------------------------------------------------\n",
      "mop-stick            4          4          18.86     \n",
      "wall                 2          3          9.98      \n",
      "floor                3          30         9.80      \n",
      "door                 2          8          8.53      \n",
      "cloth                3          64         8.13      \n",
      "cabinet              2          14         7.70      \n",
      "car                  2          38         6.22      \n",
      "move                 2          40         6.15      \n",
      "open                 2          47         5.91      \n",
      "to                   2          81         5.10      \n",
      "in                   2          99         4.81      \n",
      "from                 2          307        3.13      \n",
      "hand2                5          1088       3.05      \n",
      "pick-up              2          596        2.15      \n",
      "hand1                7          2037       1.07      \n",
      "with                 10         2446       0.25      \n",
      "verb                 11         2527       0.05      \n",
      "dobj                 9          2525       0.05      \n",
      "person               11         2545       0.00      \n",
      "pick                 1          11         FILTERED  \n",
      "put                  1          28         FILTERED  \n",
      "inside               1          5          FILTERED  \n",
      "sweep                1          6          FILTERED  \n",
      "place                1          304        FILTERED  \n",
      "on                   1          423        FILTERED  \n",
      "close                1          40         FILTERED  \n"
     ]
    }
   ],
   "source": [
    "def compute_salience(term: str, foreground: List, term_counts_bg: Counter, \n",
    "                     num_bg: int, min_freq: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    Compute discriminative salience for a term.\n",
    "    \n",
    "    Args:\n",
    "        term: The term to compute salience for\n",
    "        foreground: List of action graphs (current sample)\n",
    "        term_counts_bg: Background term counts\n",
    "        num_bg: Total number of background actions\n",
    "        min_freq: Minimum foreground frequency (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "        Salience score (higher = more discriminative), or -1 if filtered\n",
    "    \"\"\"\n",
    "    # Count foreground occurrences\n",
    "    fg_count = sum(1 for ag in foreground if term in extract_terms_from_action(ag))\n",
    "    bg_count = term_counts_bg.get(term, 0)\n",
    "    \n",
    "    # Filter by min_freq\n",
    "    if fg_count < min_freq:\n",
    "        return -1.0\n",
    "    \n",
    "    # Compute salience\n",
    "    if bg_count > 0:\n",
    "        return (1 + np.log(fg_count) ** 2) * np.log(num_bg / bg_count)\n",
    "    else:\n",
    "        return (1 + np.log(fg_count) ** 2) * np.log(num_bg)\n",
    "\n",
    "# Compute salience for all terms in foreground\n",
    "num_bg = len(background_actions)\n",
    "all_terms = set()\n",
    "for ag in foreground:\n",
    "    all_terms.update(extract_terms_from_action(ag))\n",
    "\n",
    "term_salience = []\n",
    "for term in all_terms:\n",
    "    salience = compute_salience(term, foreground, term_counts_bg, num_bg, min_freq=2)\n",
    "    term_salience.append((term, salience))\n",
    "\n",
    "# Sort by salience (descending)\n",
    "term_salience.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(\"SALIENCE SCORES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Term':<20} {'FG Count':<10} {'BG Count':<10} {'Salience':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for term, salience in term_salience:\n",
    "    fg_count = sum(1 for ag in foreground if term in extract_terms_from_action(ag))\n",
    "    bg_count = term_counts_bg.get(term, 0)\n",
    "    status = f\"{salience:.2f}\" if salience > 0 else \"FILTERED\"\n",
    "    print(f\"{term:<20} {fg_count:<10} {bg_count:<10} {status:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Extract Key Terms\n",
    "\n",
    "Key terms are the discriminative terms that will be used to detect episode boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY TERMS (sorted by salience)\n",
      "========================================\n",
      "  mop-stick: 18.86\n",
      "  wall: 9.98\n",
      "  floor: 9.80\n",
      "  door: 8.53\n",
      "  cloth: 8.13\n",
      "  cabinet: 7.70\n",
      "  car: 6.22\n",
      "  move: 6.15\n",
      "  open: 5.91\n",
      "  to: 5.10\n",
      "  in: 4.81\n",
      "  from: 3.13\n",
      "  hand2: 3.05\n",
      "  pick-up: 2.15\n",
      "  hand1: 1.07\n",
      "  with: 0.25\n",
      "  verb: 0.05\n",
      "  dobj: 0.05\n",
      "  person: 0.00\n",
      "\n",
      "Total: 19 key terms\n",
      "Filtered: 7 terms (min_freq or low salience)\n"
     ]
    }
   ],
   "source": [
    "# Filter to keep only positive salience terms\n",
    "key_terms = [(t, s) for t, s in term_salience if s > 0]\n",
    "\n",
    "print(f\"KEY TERMS (sorted by salience)\")\n",
    "print(\"=\" * 40)\n",
    "for term, salience in key_terms:\n",
    "    print(f\"  {term}: {salience:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(key_terms)} key terms\")\n",
    "print(f\"Filtered: {len(term_salience) - len(key_terms)} terms (min_freq or low salience)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Build Co-occurrence Matrix\n",
    "\n",
    "The co-occurrence matrix measures similarity between actions based on shared key terms.\n",
    "\n",
    "**Jaccard Similarity**: `|intersection| / |union|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO-OCCURRENCE MATRIX (Jaccard Similarity)\n",
      "============================================================\n",
      "Shape: (11, 11)\n",
      "\n",
      "     A0   A1   A2   A3   A4   A5   A6   A7   A8   A9   A10  \n",
      "A0     -  0.67 0.45 0.70 0.42 0.42 0.15 0.42 0.55 0.27 0.64 \n",
      "A1   0.67   -  0.45 0.70 0.42 0.55 0.15 0.42 0.42 0.36 0.50 \n",
      "A2   0.45 0.45   -  0.44 0.62 0.62 0.22 0.62 0.62 0.36 0.40 \n",
      "A3   0.70 0.70 0.44   -  0.56 0.40 0.20 0.40 0.40 0.23 0.67 \n",
      "A4   0.42 0.42 0.62 0.56   -  0.40 0.20 0.56 0.40 0.23 0.50 \n",
      "A5   0.42 0.55 0.62 0.40 0.40   -  0.20 0.56 0.75 0.45 0.36 \n",
      "A6   0.15 0.15 0.22 0.20 0.20 0.20   -  0.33 0.20 0.40 0.18 \n",
      "A7   0.42 0.42 0.62 0.40 0.56 0.56 0.33   -  0.56 0.33 0.36 \n",
      "A8   0.55 0.42 0.62 0.40 0.40 0.75 0.20 0.56   -  0.45 0.36 \n",
      "A9   0.27 0.36 0.36 0.23 0.23 0.45 0.40 0.33 0.45   -  0.31 \n",
      "A10   0.64 0.50 0.40 0.67 0.50 0.36 0.18 0.36 0.36 0.31   -  \n"
     ]
    }
   ],
   "source": [
    "def compute_cooccurrence_matrix(action_sequence: List, key_terms: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build co-occurrence matrix using Jaccard similarity.\n",
    "    \"\"\"\n",
    "    num_actions = len(action_sequence)\n",
    "    \n",
    "    # Build term presence matrix\n",
    "    term_to_idx = {t: i for i, t in enumerate(key_terms)}\n",
    "    term_presence = np.zeros((num_actions, len(key_terms)))\n",
    "    \n",
    "    for action_idx, action_graph in enumerate(action_sequence):\n",
    "        terms = extract_terms_from_action(action_graph)\n",
    "        for term in terms:\n",
    "            if term in term_to_idx:\n",
    "                term_presence[action_idx, term_to_idx[term]] = 1\n",
    "    \n",
    "    # Compute Jaccard similarity between all pairs\n",
    "    cooccur_matrix = np.zeros((num_actions, num_actions))\n",
    "    \n",
    "    for i in range(num_actions):\n",
    "        for j in range(num_actions):\n",
    "            if i != j:\n",
    "                shared = np.sum(term_presence[i] * term_presence[j])\n",
    "                total = np.sum(np.logical_or(term_presence[i], term_presence[j]))\n",
    "                if total > 0:\n",
    "                    cooccur_matrix[i, j] = shared / total\n",
    "    \n",
    "    return cooccur_matrix, term_presence\n",
    "\n",
    "# Compute co-occurrence matrix\n",
    "key_term_names = [t for t, _ in key_terms]\n",
    "cooccur_matrix, term_presence = compute_cooccurrence_matrix(action_sequence, key_term_names)\n",
    "\n",
    "print(\"CO-OCCURRENCE MATRIX (Jaccard Similarity)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {cooccur_matrix.shape}\")\n",
    "print()\n",
    "\n",
    "# Print matrix\n",
    "print(\"     \", end=\"\")\n",
    "for i in range(len(action_sequence)):\n",
    "    print(f\"A{i:<4}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(len(action_sequence)):\n",
    "    print(f\"A{i}   \", end=\"\")\n",
    "    for j in range(len(action_sequence)):\n",
    "        if i == j:\n",
    "            print(\"  -  \", end=\"\")\n",
    "        else:\n",
    "            print(f\"{cooccur_matrix[i,j]:.2f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Visualization skipped - matplotlib not available)\n"
     ]
    }
   ],
   "source": [
    "# Visualize as heatmap (if matplotlib available)\n",
    "if HAS_VIZ:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [f\"A{i}: {get_action_verb(ag)}\" for i, ag in enumerate(action_sequence)]\n",
    "    \n",
    "    sns.heatmap(cooccur_matrix, annot=True, fmt=\".2f\", cmap=\"YlOrRd\",\n",
    "                xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_title(\"Co-occurrence Matrix (Jaccard Similarity)\\nHigher = More Similar\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"(Visualization skipped - matplotlib not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Detect Episode Boundaries\n",
    "\n",
    "Episode boundaries are detected when **consecutive co-occurrence drops below threshold**.\n",
    "\n",
    "### Boundary Detection Logic\n",
    "```\n",
    "1. Compute consecutive co-occurrence scores: cooccur[i, i+1]\n",
    "2. Calculate threshold: mean - 1×std\n",
    "3. If score < threshold → START NEW EPISODE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSECUTIVE CO-OCCURRENCE SCORES\n",
      "==================================================\n",
      "  A0 → A1: 0.667\n",
      "  A1 → A2: 0.455\n",
      "  A2 → A3: 0.444\n",
      "  A3 → A4: 0.556\n",
      "  A4 → A5: 0.400\n",
      "  A5 → A6: 0.200\n",
      "  A6 → A7: 0.333\n",
      "  A7 → A8: 0.556\n",
      "  A8 → A9: 0.455\n",
      "  A9 → A10: 0.308\n",
      "\n",
      "Threshold Calculation:\n",
      "  Mean: 0.437\n",
      "  Std:  0.129\n",
      "  Threshold (mean - 1.0σ): 0.308\n",
      "\n",
      "Boundary Detection:\n",
      "  A0 → A1: 0.667 ≥ 0.308 → continue episode\n",
      "  A1 → A2: 0.455 ≥ 0.308 → continue episode\n",
      "  A2 → A3: 0.444 ≥ 0.308 → continue episode\n",
      "  A3 → A4: 0.556 ≥ 0.308 → continue episode\n",
      "  A4 → A5: 0.400 ≥ 0.308 → continue episode\n",
      "  A5 → A6: 0.200 < 0.308 → BOUNDARY DETECTED!\n",
      "  A6 → A7: 0.333 ≥ 0.308 → continue episode\n",
      "  A7 → A8: 0.556 ≥ 0.308 → continue episode\n",
      "  A8 → A9: 0.455 ≥ 0.308 → continue episode\n",
      "  A9 → A10: 0.308 < 0.308 → BOUNDARY DETECTED!\n",
      "\n",
      "==================================================\n",
      "DETECTED EPISODES: 3\n",
      "==================================================\n",
      "\n",
      "Episode 0: Actions [0, 1, 2, 3, 4, 5]\n",
      "  Verbs: ['pick-up', 'sweep', 'close', 'place', 'open', 'put']\n",
      "\n",
      "Episode 1: Actions [6, 7, 8, 9]\n",
      "  Verbs: ['move', 'open', 'pick-up', 'move']\n",
      "\n",
      "Episode 2: Actions [10]\n",
      "  Verbs: ['pick']\n"
     ]
    }
   ],
   "source": [
    "def detect_episode_boundaries(cooccur_matrix: np.ndarray, threshold_std: float = 1.0) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Detect episode boundaries based on co-occurrence shifts.\n",
    "    \n",
    "    Args:\n",
    "        cooccur_matrix: Pairwise Jaccard similarity matrix\n",
    "        threshold_std: Standard deviations below mean for boundary\n",
    "    \n",
    "    Returns:\n",
    "        List of episode groups (each is a list of action indices)\n",
    "    \"\"\"\n",
    "    num_actions = cooccur_matrix.shape[0]\n",
    "    \n",
    "    if num_actions <= 1:\n",
    "        return [[i for i in range(num_actions)]]\n",
    "    \n",
    "    # Compute consecutive co-occurrence scores\n",
    "    consecutive_scores = []\n",
    "    for i in range(num_actions - 1):\n",
    "        consecutive_scores.append(cooccur_matrix[i, i + 1])\n",
    "    \n",
    "    print(\"CONSECUTIVE CO-OCCURRENCE SCORES\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, score in enumerate(consecutive_scores):\n",
    "        print(f\"  A{i} → A{i+1}: {score:.3f}\")\n",
    "    \n",
    "    # Compute threshold\n",
    "    mean_score = np.mean(consecutive_scores)\n",
    "    std_score = np.std(consecutive_scores)\n",
    "    threshold = mean_score - threshold_std * std_score\n",
    "    \n",
    "    print(f\"\\nThreshold Calculation:\")\n",
    "    print(f\"  Mean: {mean_score:.3f}\")\n",
    "    print(f\"  Std:  {std_score:.3f}\")\n",
    "    print(f\"  Threshold (mean - {threshold_std}σ): {threshold:.3f}\")\n",
    "    \n",
    "    # Detect boundaries\n",
    "    episodes = [[0]]\n",
    "    print(f\"\\nBoundary Detection:\")\n",
    "    \n",
    "    for i, score in enumerate(consecutive_scores):\n",
    "        if score < threshold:\n",
    "            print(f\"  A{i} → A{i+1}: {score:.3f} < {threshold:.3f} → BOUNDARY DETECTED!\")\n",
    "            episodes.append([i + 1])\n",
    "        else:\n",
    "            print(f\"  A{i} → A{i+1}: {score:.3f} ≥ {threshold:.3f} → continue episode\")\n",
    "            episodes[-1].append(i + 1)\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Detect boundaries\n",
    "episode_boundaries = detect_episode_boundaries(cooccur_matrix, threshold_std=1.0)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"DETECTED EPISODES: {len(episode_boundaries)}\")\n",
    "print(\"=\" * 50)\n",
    "for i, indices in enumerate(episode_boundaries):\n",
    "    verbs = [get_action_verb(action_sequence[idx]) for idx in indices]\n",
    "    print(f\"\\nEpisode {i}: Actions {indices}\")\n",
    "    print(f\"  Verbs: {verbs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Build Structured Episodes\n",
    "\n",
    "Now we convert the detected boundaries into rich, structured episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRUCTURED EPISODES\n",
      "============================================================\n",
      "\n",
      "### Episode 0: Episode 0\n",
      "Description: Actions: pick-up, sweep, close, place, open, put\n",
      "Actions: [0, 1, 2, 3, 4, 5]\n",
      "Core structure:\n",
      "{   'agent': 'person',\n",
      "    'instruments': ['hand1', 'hand2', 'mop-stick'],\n",
      "    'primary_actions': ['pick-up', 'sweep', 'close', 'place', 'open', 'put'],\n",
      "    'primary_objects': ['mop-stick', 'floor', 'door', 'cloth'],\n",
      "    'source_locations': ['floor'],\n",
      "    'target_locations': ['car', 'floor']}\n",
      "Discriminative terms: ['mop-stick', 'floor', 'door', 'cloth', 'car']\n",
      "\n",
      "### Episode 1: Episode 1\n",
      "Description: Actions: move, open, pick-up\n",
      "Actions: [6, 7, 8, 9]\n",
      "Core structure:\n",
      "{   'agent': 'person',\n",
      "    'instruments': ['hand1'],\n",
      "    'primary_actions': ['move', 'open', 'pick-up'],\n",
      "    'primary_objects': ['cabinet', 'cloth'],\n",
      "    'source_locations': None,\n",
      "    'target_locations': ['cabinet', 'wall', 'cloth']}\n",
      "Discriminative terms: ['wall', 'cloth', 'cabinet', 'move', 'open']\n",
      "\n",
      "### Episode 2: Episode 2\n",
      "Description: Actions: pick\n",
      "Actions: [10]\n",
      "Core structure:\n",
      "{   'agent': 'person',\n",
      "    'instruments': ['hand2'],\n",
      "    'primary_actions': ['pick'],\n",
      "    'primary_objects': ['mop-stick'],\n",
      "    'source_locations': ['wall'],\n",
      "    'target_locations': None}\n",
      "Discriminative terms: ['mop-stick', 'wall', 'from', 'hand2', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Relation types for structured extraction\n",
    "VERB_RELATIONS = {\"verb\", \"verbs\"}\n",
    "OBJECT_RELATIONS = {\"dobj\", \"obj\", \"pobj\"}\n",
    "INSTRUMENT_RELATIONS = {\"with\"}\n",
    "SOURCE_RELATIONS = {\"from\"}\n",
    "TARGET_RELATIONS = {\"to\", \"on\", \"in\", \"into\", \"onto\", \"inside\"}\n",
    "\n",
    "def extract_structured_info(action_graph: List[List[str]]) -> Dict:\n",
    "    \"\"\"Extract structured information from an action graph.\"\"\"\n",
    "    info = {\n",
    "        \"agent\": None,\n",
    "        \"action\": None,\n",
    "        \"objects\": [],\n",
    "        \"instruments\": [],\n",
    "        \"source_locations\": [],\n",
    "        \"target_locations\": [],\n",
    "    }\n",
    "    \n",
    "    for triplet in action_graph:\n",
    "        if len(triplet) < 3:\n",
    "            continue\n",
    "        \n",
    "        node1, relation, node2 = triplet[0], triplet[1], triplet[2]\n",
    "        \n",
    "        if relation in VERB_RELATIONS and node1 == \"person\":\n",
    "            info[\"agent\"] = node1\n",
    "            info[\"action\"] = node2\n",
    "        elif relation in OBJECT_RELATIONS:\n",
    "            info[\"objects\"].append(node2)\n",
    "        elif relation in INSTRUMENT_RELATIONS:\n",
    "            info[\"instruments\"].append(node2)\n",
    "        elif relation in SOURCE_RELATIONS:\n",
    "            info[\"source_locations\"].append(node2)\n",
    "        elif relation in TARGET_RELATIONS:\n",
    "            info[\"target_locations\"].append(node2)\n",
    "    \n",
    "    return info\n",
    "\n",
    "def build_structured_episode(episode_id: int, action_indices: List[int], \n",
    "                              action_sequence: List, key_terms: List[Tuple[str, float]],\n",
    "                              total_episodes: int) -> Dict:\n",
    "    \"\"\"Build a structured episode from action indices.\"\"\"\n",
    "    \n",
    "    # Aggregate info from all actions\n",
    "    agent = \"person\"\n",
    "    primary_actions = []\n",
    "    primary_objects = []\n",
    "    instruments = []\n",
    "    source_locations = []\n",
    "    target_locations = []\n",
    "    \n",
    "    for idx in action_indices:\n",
    "        info = extract_structured_info(action_sequence[idx])\n",
    "        if info[\"agent\"]:\n",
    "            agent = info[\"agent\"]\n",
    "        if info[\"action\"]:\n",
    "            primary_actions.append(info[\"action\"])\n",
    "        primary_objects.extend(info[\"objects\"])\n",
    "        instruments.extend(info[\"instruments\"])\n",
    "        source_locations.extend(info[\"source_locations\"])\n",
    "        target_locations.extend(info[\"target_locations\"])\n",
    "    \n",
    "    # Deduplicate\n",
    "    primary_actions = list(dict.fromkeys(primary_actions))\n",
    "    primary_objects = list(dict.fromkeys(primary_objects))\n",
    "    instruments = list(dict.fromkeys(instruments))\n",
    "    source_locations = list(dict.fromkeys(source_locations))\n",
    "    target_locations = list(dict.fromkeys(target_locations))\n",
    "    \n",
    "    # Temporal position\n",
    "    if episode_id == 0:\n",
    "        position = \"beginning\"\n",
    "    elif episode_id == total_episodes - 1:\n",
    "        position = \"end\"\n",
    "    else:\n",
    "        position = \"middle\"\n",
    "    \n",
    "    # Get discriminative terms for this episode\n",
    "    episode_terms = set()\n",
    "    for idx in action_indices:\n",
    "        episode_terms.update(extract_terms_from_action(action_sequence[idx]))\n",
    "    discriminative_terms = [t for t, _ in key_terms if t in episode_terms][:5]\n",
    "    \n",
    "    # Build episode\n",
    "    episode = {\n",
    "        \"episode_id\": episode_id,\n",
    "        \"name\": f\"Episode {episode_id}\",  # Will be refined by LLM\n",
    "        \"description\": f\"Actions: {', '.join(primary_actions)}\",\n",
    "        \n",
    "        \"core_structure\": {\n",
    "            \"agent\": agent,\n",
    "            \"primary_actions\": primary_actions,\n",
    "            \"primary_objects\": primary_objects,\n",
    "            \"instruments\": instruments,\n",
    "            \"source_locations\": source_locations if source_locations else None,\n",
    "            \"target_locations\": target_locations if target_locations else None,\n",
    "        },\n",
    "        \n",
    "        \"time\": {\n",
    "            \"action_indices\": action_indices,\n",
    "            \"start_index\": min(action_indices),\n",
    "            \"end_index\": max(action_indices),\n",
    "            \"duration\": len(action_indices),\n",
    "        },\n",
    "        \n",
    "        \"temporal_context\": {\n",
    "            \"position\": position,\n",
    "            \"precedes_episodes\": list(range(episode_id + 1, total_episodes)) if episode_id < total_episodes - 1 else None,\n",
    "            \"follows_episodes\": list(range(episode_id)) if episode_id > 0 else None,\n",
    "        },\n",
    "        \n",
    "        \"discriminative_terms\": discriminative_terms,\n",
    "    }\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# Build structured episodes\n",
    "structured_episodes = []\n",
    "for i, action_indices in enumerate(episode_boundaries):\n",
    "    episode = build_structured_episode(\n",
    "        episode_id=i,\n",
    "        action_indices=action_indices,\n",
    "        action_sequence=action_sequence,\n",
    "        key_terms=key_terms,\n",
    "        total_episodes=len(episode_boundaries)\n",
    "    )\n",
    "    structured_episodes.append(episode)\n",
    "\n",
    "print(\"STRUCTURED EPISODES\")\n",
    "print(\"=\" * 60)\n",
    "for episode in structured_episodes:\n",
    "    print(f\"\\n### Episode {episode['episode_id']}: {episode['name']}\")\n",
    "    print(f\"Description: {episode['description']}\")\n",
    "    print(f\"Actions: {episode['time']['action_indices']}\")\n",
    "    print(f\"Core structure:\")\n",
    "    pprint(episode['core_structure'], indent=4)\n",
    "    print(f\"Discriminative terms: {episode['discriminative_terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Final Hierarchical Structure\n",
    "\n",
    "The complete hierarchical event structure ready for QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL HIERARCHICAL EVENT STRUCTURE\n",
      "======================================================================\n",
      "{\n",
      "  \"overall_goal\": \"Activity sequence\",\n",
      "  \"episodes\": [\n",
      "    {\n",
      "      \"episode_id\": 0,\n",
      "      \"name\": \"Episode 0\",\n",
      "      \"description\": \"Actions: pick-up, sweep, close, place, open, put\",\n",
      "      \"core_structure\": {\n",
      "        \"agent\": \"person\",\n",
      "        \"primary_actions\": [\n",
      "          \"pick-up\",\n",
      "          \"sweep\",\n",
      "          \"close\",\n",
      "          \"place\",\n",
      "          \"open\",\n",
      "          \"put\"\n",
      "        ],\n",
      "        \"primary_objects\": [\n",
      "          \"mop-stick\",\n",
      "          \"floor\",\n",
      "          \"door\",\n",
      "          \"cloth\"\n",
      "        ],\n",
      "        \"instruments\": [\n",
      "          \"hand1\",\n",
      "          \"hand2\",\n",
      "          \"mop-stick\"\n",
      "        ],\n",
      "        \"source_locations\": [\n",
      "          \"floor\"\n",
      "        ],\n",
      "        \"target_locations\": [\n",
      "          \"car\",\n",
      "          \"floor\"\n",
      "        ]\n",
      "      },\n",
      "      \"time\": {\n",
      "        \"action_indices\": [\n",
      "          0,\n",
      "          1,\n",
      "          2,\n",
      "          3,\n",
      "          4,\n",
      "          5\n",
      "        ],\n",
      "        \"start_index\": 0,\n",
      "        \"end_index\": 5,\n",
      "        \"duration\": 6\n",
      "      },\n",
      "      \"temporal_context\": {\n",
      "        \"position\": \"beginning\",\n",
      "        \"precedes_episodes\": [\n",
      "          1,\n",
      "          2\n",
      "        ],\n",
      "        \"follows_episodes\": null\n",
      "      },\n",
      "      \"discriminative_terms\": [\n",
      "        \"mop-stick\",\n",
      "        \"floor\",\n",
      "        \"door\",\n",
      "        \"cloth\",\n",
      "        \"car\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"episode_id\": 1,\n",
      "      \"name\": \"Episode 1\",\n",
      "      \"description\": \"Actions: move, open, pick-up\",\n",
      "      \"core_structure\": {\n",
      "        \"agent\": \"person\",\n",
      "        \"primary_actions\": [\n",
      "          \"move\",\n",
      "          \"open\",\n",
      "          \"pick-up\"\n",
      "        ],\n",
      "        \"primary_objects\": [\n",
      "          \"cabinet\",\n",
      "          \"cloth\"\n",
      "        ],\n",
      "        \"instruments\": [\n",
      "          \"hand1\"\n",
      "        ],\n",
      "        \"source_locations\": null,\n",
      "        \"target_locations\": [\n",
      "          \"cabinet\",\n",
      "          \"wall\",\n",
      "          \"cloth\"\n",
      "        ]\n",
      "      },\n",
      "      \"time\": {\n",
      "        \"action_indices\": [\n",
      "          6,\n",
      "          7,\n",
      "          8,\n",
      "          9\n",
      "        ],\n",
      "        \"start_index\": 6,\n",
      "        \"end_index\": 9,\n",
      "        \"duration\": 4\n",
      "      },\n",
      "      \"temporal_context\": {\n",
      "        \"position\": \"middle\",\n",
      "        \"precedes_episodes\": [\n",
      "          2\n",
      "        ],\n",
      "        \"follows_episodes\": [\n",
      "          0\n",
      "        ]\n",
      "      },\n",
      "      \"discriminative_terms\": [\n",
      "        \"wall\",\n",
      "        \"cloth\",\n",
      "        \"cabinet\",\n",
      "        \"move\",\n",
      "        \"open\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"episode_id\": 2,\n",
      "      \"name\": \"Episode 2\",\n",
      "      \"description\": \"Actions: pick\",\n",
      "      \"core_structure\": {\n",
      "        \"agent\": \"person\",\n",
      "        \"primary_actions\": [\n",
      "          \"pick\"\n",
      "        ],\n",
      "        \"primary_objects\": [\n",
      "          \"mop-stick\"\n",
      "        ],\n",
      "        \"instruments\": [\n",
      "          \"hand2\"\n",
      "        ],\n",
      "        \"source_locations\": [\n",
      "          \"wall\"\n",
      "        ],\n",
      "        \"target_locations\": null\n",
      "      },\n",
      "      \"time\": {\n",
      "        \"action_indices\": [\n",
      "          10\n",
      "        ],\n",
      "        \"start_index\": 10,\n",
      "        \"end_index\": 10,\n",
      "        \"duration\": 1\n",
      "      },\n",
      "      \"temporal_context\": {\n",
      "        \"position\": \"end\",\n",
      "        \"precedes_episodes\": null,\n",
      "        \"follows_episodes\": [\n",
      "          0,\n",
      "          1\n",
      "        ]\n",
      "      },\n",
      "      \"discriminative_terms\": [\n",
      "        \"mop-stick\",\n",
      "        \"wall\",\n",
      "        \"from\",\n",
      "        \"hand2\",\n",
      "        \"with\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"sample_id\": \"19cc4e42-39bb-41f9-b9de-9f2940eed6a2\",\n",
      "    \"total_actions\": 11,\n",
      "    \"total_episodes\": 3,\n",
      "    \"method\": \"EpiMine (co-occurrence boundary detection)\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Build final hierarchy\n",
    "hierarchy = {\n",
    "    \"overall_goal\": \"Activity sequence\",  # Would be refined by LLM\n",
    "    \"episodes\": structured_episodes,\n",
    "    \"metadata\": {\n",
    "        \"sample_id\": sample[\"data_id\"],\n",
    "        \"total_actions\": len(action_sequence),\n",
    "        \"total_episodes\": len(structured_episodes),\n",
    "        \"method\": \"EpiMine (co-occurrence boundary detection)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"FINAL HIERARCHICAL EVENT STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "print(json.dumps(hierarchy, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Visualize Timeline\n",
    "\n",
    "Format the hierarchy for QA prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Overall Goal\n",
      "Activity sequence\n",
      "\n",
      "## Activity Timeline\n",
      "\n",
      "### Episode 0: Episode 0\n",
      "Actions: pick-up, sweep, close, place, open, put\n",
      "\n",
      "**Structure:**\n",
      "- Agent: person\n",
      "- Actions: pick-up, sweep, close, place, open, put\n",
      "- Objects: mop-stick, floor, door, cloth\n",
      "- Instruments: hand1, hand2, mop-stick\n",
      "- From: floor\n",
      "- To: car, floor\n",
      "- Time: Actions [0, 1, 2, 3, 4, 5] (duration: 6)\n",
      "- Key terms: mop-stick, floor, door, cloth, car\n",
      "\n",
      "**Actions in this episode:**\n",
      "- Action 0 (pick-up): ['pick-up', 'with', 'hand1'] ['pick-up', 'with', 'hand2'] ['mop-stick', 'from', 'floor'] ['person', 'verb', 'pick-up'] ['pick-up', 'dobj', 'mop-stick']\n",
      "- Action 1 (sweep): ['sweep', 'with', 'hand1'] ['sweep', 'with', 'hand2'] ['sweep', 'with', 'mop-stick'] ['sweep', 'dobj', 'floor'] ['sweep', 'in', 'car'] ['person', 'verb', 'sweep']\n",
      "- Action 2 (close): ['close', 'with', 'hand1'] ['close', 'dobj', 'door'] ['person', 'verb', 'close']\n",
      "- Action 3 (place): ['place', 'with', 'hand2'] ['place', 'dobj', 'mop-stick'] ['place', 'on', 'floor'] ['person', 'verb', 'place']\n",
      "- Action 4 (open): ['open', 'dobj', 'door'] ['open', 'with', 'hand2'] ['person', 'verb', 'open']\n",
      "- Action 5 (put): ['put', 'dobj', 'cloth'] ['put', 'inside', 'car'] ['put', 'with', 'hand1'] ['person', 'verb', 'put']\n",
      "\n",
      "### Episode 1: Episode 1\n",
      "Actions: move, open, pick-up\n",
      "\n",
      "**Structure:**\n",
      "- Agent: person\n",
      "- Actions: move, open, pick-up\n",
      "- Objects: cabinet, cloth\n",
      "- Instruments: hand1\n",
      "- To: cabinet, wall, cloth\n",
      "- Time: Actions [6, 7, 8, 9] (duration: 4)\n",
      "- Key terms: wall, cloth, cabinet, move, open\n",
      "\n",
      "**Actions in this episode:**\n",
      "- Action 6 (move): ['person', 'verb', 'move'] ['move', 'to', 'cabinet']\n",
      "- Action 7 (open): ['open', 'dobj', 'cabinet'] ['open', 'with', 'hand1'] ['person', 'verb', 'open']\n",
      "- Action 8 (pick-up): ['pick-up', 'with', 'hand1'] ['pick-up', 'dobj', 'cloth'] ['person', 'verb', 'pick-up']\n",
      "- Action 9 (move): ['move', 'to', 'wall'] ['hand1', 'in', 'cloth'] ['move', 'with', 'hand1'] ['person', 'verb', 'move']\n",
      "\n",
      "### Episode 2: Episode 2\n",
      "Actions: pick\n",
      "\n",
      "**Structure:**\n",
      "- Agent: person\n",
      "- Actions: pick\n",
      "- Objects: mop-stick\n",
      "- Instruments: hand2\n",
      "- From: wall\n",
      "- Time: Actions [10] (duration: 1)\n",
      "- Key terms: mop-stick, wall, from, hand2, with\n",
      "\n",
      "**Actions in this episode:**\n",
      "- Action 10 (pick): ['pick', 'with', 'hand2'] ['pick', 'from', 'wall'] ['person', 'verb', 'pick'] ['pick', 'dobj', 'mop-stick']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_timeline(hierarchy: Dict, action_sequence: List) -> str:\n",
    "    \"\"\"Format hierarchy as a readable timeline.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(f\"## Overall Goal\\n{hierarchy['overall_goal']}\\n\")\n",
    "    lines.append(\"## Activity Timeline\\n\")\n",
    "    \n",
    "    for episode in hierarchy[\"episodes\"]:\n",
    "        ep_id = episode[\"episode_id\"]\n",
    "        name = episode[\"name\"]\n",
    "        desc = episode[\"description\"]\n",
    "        core = episode[\"core_structure\"]\n",
    "        time_info = episode[\"time\"]\n",
    "        disc_terms = episode.get(\"discriminative_terms\", [])\n",
    "        \n",
    "        lines.append(f\"### Episode {ep_id}: {name}\")\n",
    "        lines.append(f\"{desc}\\n\")\n",
    "        \n",
    "        lines.append(\"**Structure:**\")\n",
    "        lines.append(f\"- Agent: {core['agent']}\")\n",
    "        lines.append(f\"- Actions: {', '.join(core['primary_actions'])}\")\n",
    "        if core['primary_objects']:\n",
    "            lines.append(f\"- Objects: {', '.join(core['primary_objects'])}\")\n",
    "        if core['instruments']:\n",
    "            lines.append(f\"- Instruments: {', '.join(core['instruments'])}\")\n",
    "        if core['source_locations']:\n",
    "            lines.append(f\"- From: {', '.join(core['source_locations'])}\")\n",
    "        if core['target_locations']:\n",
    "            lines.append(f\"- To: {', '.join(core['target_locations'])}\")\n",
    "        lines.append(f\"- Time: Actions {time_info['action_indices']} (duration: {time_info['duration']})\")\n",
    "        if disc_terms:\n",
    "            lines.append(f\"- Key terms: {', '.join(disc_terms)}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        lines.append(\"**Actions in this episode:**\")\n",
    "        for idx in time_info[\"action_indices\"]:\n",
    "            verb = get_action_verb(action_sequence[idx])\n",
    "            triplets = \" \".join(str(t) for t in action_sequence[idx])\n",
    "            lines.append(f\"- Action {idx} ({verb}): {triplets}\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Generate formatted timeline\n",
    "timeline = format_timeline(hierarchy, action_sequence)\n",
    "print(timeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated the complete EpiMine pipeline:\n",
    "\n",
    "1. **Load data**: Scene graph triplets representing actions\n",
    "2. **Build background**: All actions from dataset for salience computation\n",
    "3. **Compute salience**: Identify discriminative terms using TF-IDF-like formula\n",
    "4. **Extract key terms**: Filter by `min_freq` threshold\n",
    "5. **Build co-occurrence matrix**: Jaccard similarity between actions\n",
    "6. **Detect boundaries**: `mean - 1σ` threshold on consecutive scores\n",
    "7. **Build structured episodes**: Rich format with temporal context\n",
    "8. **Format for QA**: Human-readable timeline\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Default | Effect |\n",
    "|-----------|---------|--------|\n",
    "| `min_freq` | 2 | Filter rare terms |\n",
    "| `threshold_std` | 1.0 | Boundary sensitivity |\n",
    "\n",
    "### What's NOT Used in SGQA (vs Original EpiMine)\n",
    "\n",
    "- **Term expansion (cosine ≥ 0.75)**: Not needed — fixed vocabulary, no synonyms\n",
    "- **Cross-document clustering**: Not needed — single sample processing\n",
    "- **Embedding similarity**: Not needed — triplets are structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsg-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
